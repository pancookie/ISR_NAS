{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee2b4d-35bb-4ebd-ad3e-972cf1f998e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    SLNN/DNN code for ISR \n",
    "    manual code which you can customize the network details, \n",
    "    such as network architecture, learning rate, etc.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa830756-210c-4d26-b02c-f28398c6d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import multiprocess as mp\n",
    "# ML related \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "# plot related\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "# from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b759a-b057-400b-ae2f-ff32dd17448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% paths of source files [please adjust accordingly]\n",
    "\n",
    "path_work_fd = './data'\n",
    "path_figsav = './nel_daily'\n",
    "path_mdsav = './md_res'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef7103-c6eb-477d-891f-251b4654eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% parameters \n",
    "parms_inv = ['year', 'dayno', 'ut', 'f10.7', 'ap3', 'nel']\n",
    "parms_input = ['year', 'doy_sin', 'doy_cos', 'ut_sin', 'ut_cos', 'f10.7', 'ap3']\n",
    "parms_inp_norm = ['year', 'f10.7', 'ap3']\n",
    "parms_output = ['nel']\n",
    "\n",
    "# split on training/val/test sets\n",
    "list_yr_val = [2010, 2015]\n",
    "list_yr_test = [2007, 2012]\n",
    "\n",
    "# location of Millstone ISR\n",
    "isr_lat, isr_lon = 42.61, 288.51\n",
    "diff_utslt = abs(isr_lon-360)/360*24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0ab67-a05a-4675-bed2-385f35b3f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% confine to specified GPU device [please adjust accordingly]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22856f53-1dd7-4ea8-9b7a-165987e9c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% prepare data [please adjust accordingly] \n",
    "'''\n",
    "    In our study, we had processed data in UT in order to compare with SLT, \n",
    "    you can customize for your convenience\n",
    "    \n",
    "'''\n",
    "df_isr = pd.read_feather(os.path.join(path_work_fd, 'isr_hourly_ver1.lz4'))\n",
    "\n",
    "df_isr.index = pd.to_datetime(df_isr['timestamp_ut'])\n",
    "# prepare all the needed input parameters\n",
    "df_isr['year'] = df_isr.index.year\n",
    "df_isr['dayno'] = df_isr.index.dayofyear\n",
    "df_isr['ut'] = df_isr.index.hour+df_isr.index.minute/60\n",
    "\n",
    "# get cyclic on dayno and ut\n",
    "doy_sin = (np.sin(df_isr['dayno']/365 * 2*np.pi)+1)/2\n",
    "doy_cos = (np.cos(df_isr['dayno']/365 * 2*np.pi)+1)/2\n",
    "ut_sin = (np.sin(df_isr['ut']/24 * 2*np.pi)+1)/2\n",
    "ut_cos = (np.cos(df_isr['ut']/24 * 2*np.pi)+1)/2\n",
    "\n",
    "df_isr_norm = df_isr.copy(True)\n",
    "df_isr_norm['doy_sin'] = doy_sin\n",
    "df_isr_norm['doy_cos'] = doy_cos\n",
    "df_isr_norm['ut_sin'] = ut_sin\n",
    "df_isr_norm['ut_cos'] = ut_cos\n",
    "# normalize year, F10.7 and Ap3\n",
    "df_isr_norm.loc[:, parms_inp_norm] /= df_isr.loc[:, parms_inp_norm].max()\n",
    "\n",
    "# split training/validation/test\n",
    "df_isr_train_norm = df_isr_norm.loc[~df_isr_norm.index.year.isin(list_yr_val+list_yr_test)]\n",
    "df_isr_val_norm = df_isr_norm.loc[df_isr_norm.index.year.isin(list_yr_val)]\n",
    "df_isr_test_norm = df_isr_norm.loc[df_isr_norm.index.year.isin(list_yr_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aabbe2-6c39-4c5a-8c13-607202cb42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the network [please adjust accordingly]\n",
    "input_size = len(parms_input)\n",
    "hidden_sizes = [18,36,38,54]\n",
    "output_size = len(parms_output)\n",
    "inp_hid = [input_size]+hidden_sizes\n",
    "\n",
    "md_name = 'dnn_ak_run6'\n",
    "bs_size = 64\n",
    "lr_ = 5e-05\n",
    "num_epoch = 8000\n",
    "\n",
    "# make dir\n",
    "os.makedirs(os.path.join(path_mdsav, md_name), exist_ok=True)\n",
    "\n",
    "# make DataLoader according to batch size\n",
    "train_X = torch.tensor(df_isr_train_norm.loc[:, parms_input].values, dtype=torch.float).to(device)\n",
    "train_y = torch.tensor(df_isr_train_norm.loc[:, parms_output].values, dtype=torch.float).to(device)\n",
    "\n",
    "val_X = torch.tensor(df_isr_val_norm.loc[:, parms_input].values, dtype=torch.float).to(device)\n",
    "val_y = torch.tensor(df_isr_val_norm.loc[:, parms_output].values, dtype=torch.float).to(device)\n",
    "\n",
    "test_X = torch.tensor(df_isr_test_norm.loc[:, parms_input].values, dtype=torch.float).to(device)\n",
    "test_y = torch.tensor(df_isr_test_norm.loc[:, parms_output].values, dtype=torch.float).to(device)\n",
    "\n",
    "# load into DataLoader\n",
    "train_loader = DataLoader(list(zip(train_X, train_y)), shuffle=True, batch_size=bs_size)\n",
    "val_loader = DataLoader(list(zip(val_X, val_y)), shuffle=True, batch_size=bs_size)\n",
    "test_loader = DataLoader(list(zip(test_X, test_y)), shuffle=True, batch_size=bs_size)\n",
    "\n",
    "\n",
    "# make the structure\n",
    "list_dense = [(f'dense_{idx}', nn.Linear(inp_hid[idx], inp_hid[idx+1])) for idx, neu_num in enumerate(inp_hid[:-1])]\n",
    "list_actv = [(f'activation_{idx}', nn.ReLU()) for idx in range(len(inp_hid)-1)]\n",
    "dict_nn_structure = list(sum(list(zip(list_dense, list_actv)), ()))+[('output', nn.Linear(hidden_sizes[-1], 1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdbf09-440b-4396-8463-701dab8f89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the NN\n",
    "model = nn.Sequential(OrderedDict(dict_nn_structure))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f130d-a5d2-43b1-aebc-7055cf5afc0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continue the training\n",
    "torch.manual_seed(49)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)\n",
    "model.apply(weights_init)\n",
    "\n",
    "loss_l1 = nn.L1Loss()\n",
    "loss_mse = nn.MSELoss()\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_)\n",
    "\n",
    "list_loss_train, list_loss_val, list_loss_test = [], [], []\n",
    "\n",
    "# iterate over the epochs ## save loss.item() only\n",
    "stime = time.time()\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    r_loss_train, r_rmse_train, r_re_train = 0, 0, 0\n",
    "    # on training set\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss_train = loss_l1(y_pred, y_batch)\n",
    "        r_loss_train += float(loss_train.item())\n",
    "        r_rmse_train += float(torch.sqrt(loss_mse(y_pred, y_batch)).item())\n",
    "        r_re_train += float(torch.mean(torch.abs(y_pred-y_batch)/y_batch).item())\n",
    "        # update the network\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    # averaged training loss over all batches\n",
    "    loss_avg_train = r_loss_train / len(train_loader)\n",
    "    rmse_avg_train = r_rmse_train / len(train_loader)\n",
    "    re_avg_train = r_re_train / len(train_loader)\n",
    "        \n",
    "    # on val set and test set on the whole set\n",
    "    r_loss_val, r_rmse_val, r_re_val = 0, 0, 0\n",
    "    r_loss_test, r_rmse_test, r_re_test = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss_val = loss_l1(y_pred, y_batch).item()\n",
    "            r_loss_val += float(loss_val)\n",
    "            r_rmse_val += float(torch.sqrt(loss_mse(y_pred, y_batch)).item())\n",
    "            r_re_val += float(torch.mean(torch.abs(y_pred-y_batch)/y_batch).item())            \n",
    "            \n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss_test = loss_l1(y_pred, y_batch).item()\n",
    "            r_loss_test += float(loss_test)\n",
    "            r_rmse_test += float(torch.sqrt(loss_mse(y_pred, y_batch)).item())\n",
    "            r_re_test += float(torch.mean(torch.abs(y_pred-y_batch)/y_batch).item())            \n",
    "        \n",
    "    loss_avg_val = r_loss_val / len(val_loader)\n",
    "    rmse_avg_val = r_rmse_val / len(val_loader)\n",
    "    re_avg_val = r_re_val / len(val_loader)\n",
    "    \n",
    "    loss_avg_test = r_loss_test / len(test_loader)\n",
    "    rmse_avg_test = r_rmse_test / len(test_loader)\n",
    "    re_avg_test = r_re_test / len(test_loader)\n",
    "\n",
    "    errs = {}\n",
    "    errs['mae'], errs['rmse'], errs['re'] = {}, {}, {}\n",
    "    errs['mae']['train'] = loss_avg_train\n",
    "    errs['mae']['val'] = loss_avg_val\n",
    "    errs['mae']['test'] = loss_avg_test\n",
    "\n",
    "    errs['rmse']['train'] = rmse_avg_train\n",
    "    errs['rmse']['val'] = rmse_avg_val\n",
    "    errs['rmse']['test'] = rmse_avg_test\n",
    "\n",
    "    errs['re']['train'] = re_avg_train\n",
    "    errs['re']['val'] = re_avg_val\n",
    "    errs['re']['test'] = re_avg_test\n",
    "    # display the info every 100 epochs\n",
    "    if e%100 == 0:\n",
    "        print(f'# Epoch: {e}, Training loss: {loss_avg_train}, Val loss: {loss_avg_val}, Test loss: {loss_avg_test}')\n",
    "\n",
    "    # save the info\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': e,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': errs,\n",
    "    }, os.path.join(path_mdsav, md_name, f'epc_{e}.pt'))\n",
    "    \n",
    "etime = time.time()\n",
    "print((etime-stime)/60.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d7f3a-2a4a-43c4-a151-2f58911d6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((etime-stime)/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5493e-43f8-44f2-9c99-18920a702857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f9487-25dc-4c35-9536-79d5cb05a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd188313-f1fa-4a4b-a171-c95766939ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
